{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import joblib\n",
    "from os import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from datetime import datetime, timedelta, date\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn import ensemble\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import gaussian_process\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import neural_network\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score,  f1_score, log_loss\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds = datetime.strptime('2017-01-01', '%Y-%m-%d')\n",
    "df=pd.read_csv('data/to_train.csv')\n",
    "df['marketTime']=pd.to_datetime(df['marketTime'])\n",
    "df.sort_values(by='marketTime', inplace=True)\n",
    "df['rid']=df.course.str.lower().replace(regex=True,to_replace=r'\\\\W|\\s',value=r'')+df.marketTime.dt.strftime('%Y%m%d%H%M')\n",
    "df=df.fillna(0)\n",
    "\n",
    "df_pred=pd.read_csv('data/topred.csv')\n",
    "df_pred=df_pred.fillna(0)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred=pd.read_csv('data/topred.csv')\n",
    "df_pred=df_pred.fillna(0)\n",
    "cols=['rid','course', 'marketTime', 'horseName', 'position','res_win', 'res_place', 'runners', 'ncond', 'metric', 'class','decimalPrice', 'age', 'RPR', 'TR', 'OR', 'weight','age_rank', 'decimalPrice_rank','weight_rank', 'RPR_rank', 'TR_rank','OR_rank', 'res_win_h_avg_rank', 'res_place_h_avg_rank','decimalPrice_diff_h_avg_rank', 'position_diff_h_avg_rank','res_win_t_avg_rank', 'res_place_t_avg_rank','decimalPrice_diff_t_avg_rank', 'position_diff_t_avg_rank','res_win_j_avg_rank', 'res_place_j_avg_rank','decimalPrice_diff_j_avg_rank', 'position_diff_j_avg_rank','metric_h_avg', 'res_win_h_avg','res_place_h_avg','decimalPrice_diff_h_avg', 'RPR_diff_h_avg', 'TR_diff_h_avg','OR_diff_h_avg', 'position_diff_h_avg', 'metric_t_avg', 'res_win_t_avg','res_place_t_avg', 'decimalPrice_diff_t_avg', 'RPR_diff_t_avg','TR_diff_t_avg', 'OR_diff_t_avg', 'position_diff_t_avg', 'metric_j_avg','res_win_j_avg', 'res_place_j_avg', 'decimalPrice_diff_j_avg','RPR_diff_j_avg', 'TR_diff_j_avg', 'OR_diff_j_avg','position_diff_j_avg', 'res_win_h_avg_diff', 'res_place_h_avg_diff','decimalPrice_diff_h_avg_diff', 'position_diff_h_avg_diff','res_win_t_avg_diff', 'res_place_t_avg_diff','decimalPrice_diff_t_avg_diff', 'position_diff_t_avg_diff','res_win_j_avg_diff', 'res_place_j_avg_diff','decimalPrice_diff_j_avg_diff', 'position_diff_j_avg_diff', 'age_diff','decimalPrice_diff', 'weight_diff', 'RPR_diff', 'TR_diff', 'OR_diff']\n",
    "df=df[cols]\n",
    "df_pred=df_pred[['date']+cols]\n",
    "\n",
    "\n",
    "# Get all RIDs\n",
    "rids=df.rid.unique()\n",
    "#validate=rids[-10000:]\n",
    "#rids=rids[:-10000]\n",
    "np.random.shuffle(rids)\n",
    "# Get three RIDs lists\n",
    "#train, test, validate = np.split(rids, [int(.7*len(rids)), int(.85*len(rids))]) \n",
    "train, test = np.split(rids, [int(.9*len(rids))]) \n",
    "# Split dataframe on parts\n",
    "train_df=df[df['rid'].isin(train)]\n",
    "test_df=df[df['rid'].isin(test)]\n",
    "#validate_df=df[df['rid'].isin(validate)]\n",
    "validate_df=df_pred\n",
    "\n",
    "# Catogorical are columns with rank and three others\n",
    "cols_categorical=[col for col in df.columns if '_rank' in col]+['ncond', 'class']\n",
    "\n",
    "# Filter out cols_categorical\n",
    "cols=[col for col in df.columns if not col in cols_categorical]\n",
    "# Numerical are columns with avg and diff\n",
    "cols_numerical=[col for col in cols if ('_avg' in col) or ('_diff' in col)]\n",
    "# ...and some others\n",
    "#cols_numerical=cols_numerical+['RPR', 'TR', 'OR', 'win_drift']\n",
    "cols_numerical=cols_numerical+ ['RPR', 'TR','OR','decimalPrice']\n",
    "\n",
    "# The rest columns \n",
    "cols=[col for col in cols if not col in cols_numerical]\n",
    "\n",
    "cols_descr=['rid','course','marketTime', 'horseName', 'runners','decimalPrice','res_win', 'res_place']\n",
    "\n",
    "# Prepare data\n",
    "train_x=train_df[cols_categorical+cols_numerical].values\n",
    "train_y=train_df['res_win'].astype(int).values\n",
    "test_x=test_df[cols_categorical+cols_numerical].values\n",
    "test_y=test_df['res_win'].astype(int).values\n",
    "validate_x=validate_df[cols_categorical+cols_numerical].values\n",
    "validate_y=validate_df['res_win'].astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = {}\n",
    "clfs['gbc'] = {'clf': ensemble.GradientBoostingClassifier(), 'name': 'GradientBoostingClassifier'}\n",
    "\n",
    "clfs['xgb'] = {'clf': XGBClassifier(), 'name': \"XGBClassifier\"}\n",
    "clfs['GPC'] = {'clf': gaussian_process.GaussianProcessClassifier(), 'name': 'GaussianProcess'}\n",
    "clfs['cb'] = {'clf': CatBoostClassifier(), 'name': 'CBC'}\n",
    "clfs['lr'] = {'clf': linear_model.LogisticRegression(), 'name': 'LogisticRegression'}\n",
    "\n",
    "parameters = {'C':[1],'tol':[0.0001],'solver': ['newton-cg'], 'multi_class': ['multinomial']}\n",
    "clfs['lrgrid'] = {'clf': GridSearchCV(linear_model.LogisticRegression(), parameters), 'name': 'LogisticRegression'}\n",
    "\n",
    "parameters = {'n_estimators':np.arange(64, 1024, step=64)}\n",
    "clfs['rfgrid'] = {'clf': GridSearchCV(ensemble.RandomForestClassifier(), parameters), 'name': 'Random Forest'}\n",
    "\n",
    "parameters = {'kernel':['linear', 'sigmoid', 'poly', 'rbf'], 'gamma':np.linspace(0.0,2.0,num=21),'C': np.linspace(0.5,1.5,num=11)}\n",
    "clfs['svcgrid'] = {'clf': GridSearchCV(svm.SVC(), parameters), 'name': 'SVC with GridSearch'}\n",
    "clfs['svc'] = {'clf': svm.SVC(probability=True), 'name': 'SVC'}\n",
    "parameters = {'n_estimators':np.arange(3, 11, step=2)}\n",
    "clfs['adagrid'] = {'clf': GridSearchCV(ensemble.AdaBoostClassifier(), parameters), 'name': 'AdaBoost'}\n",
    "\n",
    "#clfs['nb'] = {'clf': naive_bayes.GaussianNB(), 'name':'GaussianNaiveBayes'}\n",
    "clfs['mlp'] = {'clf': neural_network.MLPClassifier(), 'name': 'MLP'}\n",
    "#clfs['knngrid'] = {'clf': neighbors.KNeighborsClassifier(), 'name': 'KNN'}\n",
    "clfs['tr'] = {'clf': tree.DecisionTreeClassifier(), 'name':'DecisionTree'}\n",
    "#clfs['extr'] = {'clf': ensemble.ExtraTreesClassifier(), 'name':'ExtraTree'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['accuracy', 'neg_log_loss']\n",
    "def process_clf(clf):\n",
    "    model=clfs[clf]['clf']\n",
    "    scores = cross_validate(model, test_x, test_y, scoring=scoring, cv=10)\n",
    "    sorted(scores.keys())\n",
    "    model=model.fit(test_x, test_y)\n",
    "    \n",
    "    pred=model.predict(test_x)\n",
    "    prob=model.predict_proba(test_x)[:,1]\n",
    "    acc=accuracy_score(test_y, pred)\n",
    "    loss=log_loss(test_y,prob)\n",
    "\n",
    "    predv=model.predict(validate_x)\n",
    "    probv=model.predict_proba(validate_x)[:,1]\n",
    "    accv=accuracy_score(validate_y, predv)\n",
    "    lossv=log_loss(validate_y,probv)\n",
    "    print('{}: train {:0.4f}, logloss:{:0.4f}; test {:0.4f}, logloss:{:0.4f}; validate {:0.4f}, logloss:{:0.4f}'.format(clfs[clf]['name'],scores['test_accuracy'].mean(),scores['test_neg_log_loss'].mean(),acc,loss,accv,lossv))\n",
    "    descr=test_df[cols_descr].reset_index(drop=True)\n",
    "    calc_profit(clf,1,descr,prob)\n",
    "    descr=validate_df[cols_descr].reset_index(drop=True)\n",
    "    calc_profit(clf,2,descr,probv)\n",
    "    return prob, probv\n",
    "\n",
    "scoring = ['accuracy', 'neg_log_loss']\n",
    "def pred_clf(clf):\n",
    "    #scores = cross_validate(model, test_x, test_y, scoring=scoring, cv=10)\n",
    "    #sorted(scores.keys())\n",
    "    fn=f'models/{clf}.sav'\n",
    "    if path.exists(fn):\n",
    "        print('Using the saved model')\n",
    "        model = joblib.load(fn)\n",
    "    else:\n",
    "        model=clfs[clf]['clf']\n",
    "        print('Have to train a model')\n",
    "        model=model.fit(test_x, test_y)\n",
    "        joblib.dump(model, fn)\n",
    "    pred=model.predict(test_x)\n",
    "    prob=model.predict_proba(test_x)[:,1]\n",
    "    acc=accuracy_score(test_y, pred)\n",
    "    loss=log_loss(test_y,prob)\n",
    "\n",
    "    predv=model.predict(validate_x)\n",
    "    probv=model.predict_proba(validate_x)[:,1]\n",
    "    print('{}: test {:0.4f}, logloss:{:0.4f};'.format(clfs[clf]['name'],acc,loss))\n",
    "    descr=test_df[cols_descr].reset_index(drop=True)\n",
    "    calc_profit(clf,1,descr,prob)\n",
    "    descr=validate_df[['date']+cols_descr].reset_index(drop=True)\n",
    "    calc_pred(clf,descr,probv)\n",
    "    return prob, probv\n",
    "\n",
    "def calc_profit(clf,var,descr,prob):\n",
    "    descr=pd.concat([descr,pd.DataFrame(prob, columns=['prob'])], axis=1)\n",
    "    descr['probsum']=descr.groupby(['rid'])['prob'].transform('sum')\n",
    "    descr['prob']=descr['prob']/descr['probsum']\n",
    "    descr['diff']=(descr['prob']-descr['decimalPrice'])*10\n",
    "    descr['odds']=1/descr['decimalPrice']\n",
    "    descr['C']=1\n",
    "    descr['prf_win']=np.where(descr['res_win']==1,descr['odds']-1,-1)\n",
    "    descr['prf_place']=np.where(descr['res_place']==1,(descr['odds']-1)/5,-1)\n",
    "    descr['prf_ew']=descr['prf_win']/2+descr['prf_place']/2\n",
    "    descr.to_csv(f'data/try_{clf}{var}.csv')\n",
    "\n",
    "def calc_pred(clf,descr,prob):\n",
    "    descr=pd.concat([descr,pd.DataFrame(prob, columns=['prob'])], axis=1)\n",
    "    descr['probsum']=descr.groupby(['rid'])['prob'].transform('sum')\n",
    "    descr['prob']=descr['prob']/descr['probsum']\n",
    "    descr['diff']=(descr['prob']-descr['decimalPrice'])*10\n",
    "    descr['odds']=1/descr['decimalPrice']\n",
    "    descr['C']=1\n",
    "    descr['prf_win']=np.where(descr['res_win']==1,descr['odds']-1,-1)\n",
    "    descr['prf_place']=np.where(descr['res_place']==1,(descr['odds']-1)/5,-1)\n",
    "    descr['prf_ew']=descr['prf_win']/2+descr['prf_place']/2\n",
    "    descr.to_csv(f'data/today_{clf}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Using the saved model\nGradientBoostingClassifier: test 0.9449, logloss:0.1347;\n"
    }
   ],
   "source": [
    "prob,probv=pred_clf('gbc') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Have to train a model\nXGBClassifier: test 0.9798, logloss:0.0637;\n"
    }
   ],
   "source": [
    "prob,probv=pred_clf('xgb') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred=pd.read_csv('data/topred.csv')\n",
    "df_pred=df_pred.fillna(0)\n",
    "validate_df=df_pred\n",
    "\n",
    "# Catogorical are columns with rank and three others\n",
    "cols_categorical=[col for col in validate_df.columns if '_rank' in col]+['ncond', 'class']\n",
    "\n",
    "validate_df[cols_categorical]=validate_df[cols_categorical].astype(int)\n",
    "\n",
    "# Filter out cols_categorical\n",
    "cols=[col for col in validate_df.columns if not col in cols_categorical]\n",
    "# Numerical are columns with avg and diff\n",
    "cols_numerical=[col for col in cols if ('_avg' in col) or ('_diff' in col)]\n",
    "# ...and some others\n",
    "#cols_numerical=cols_numerical+['RPR', 'TR', 'OR', 'win_drift']\n",
    "cols_numerical=cols_numerical+ ['RPR', 'TR','OR','decimalPrice']\n",
    "\n",
    "# The rest columns \n",
    "cols=[col for col in cols if not col in cols_numerical]\n",
    "\n",
    "cols_descr=['rid','course','marketTime', 'horseName', 'runners','decimalPrice','res_win', 'res_place']\n",
    "\n",
    "# Prepare data\n",
    "validate_x=validate_df[cols_categorical+cols_numerical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_clf(clf):\n",
    "    #scores = cross_validate(model, test_x, test_y, scoring=scoring, cv=10)\n",
    "    #sorted(scores.keys())\n",
    "    if clf=='cb':\n",
    "        fn='models/horses007.cbm'\n",
    "        model=CatBoostClassifier()\n",
    "        model.load_model(fn)\n",
    "    else:\n",
    "        fn=f'models/{clf}.sav'\n",
    "        model = joblib.load(fn)\n",
    "    predv=model.predict(validate_x)\n",
    "    probv=model.predict_proba(validate_x)[:,1]\n",
    "    descr=validate_df[cols_descr].reset_index(drop=True)\n",
    "    calc_pred(clf,descr,probv)\n",
    "    return probv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prob,probv=process_clf('lr') \n",
    "probv=pred_clf('cb') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gbc=pd.read_csv('data/today_gbc.csv')\n",
    "df_xgb=pd.read_csv('data/today_xgb.csv')\n",
    "df_cb=pd.read_csv('data/today_cb.csv')\n",
    "df_gbc=df_gbc.drop_duplicates(subset=['course','marketTime','horseName'], keep='last')\n",
    "df_xgb=df_xgb.drop_duplicates(subset=['course','marketTime','horseName'], keep='last')\n",
    "df_cb=df_cb.drop_duplicates(subset=['course','marketTime','horseName'], keep='last')\n",
    "df_gbc.rename(columns={'prob':'prob_gbc','diff':'diff_gbc'}, inplace=True)\n",
    "df_xgb.rename(columns={'prob':'prob_xgb','diff':'diff_xgb'}, inplace=True)\n",
    "df_cb.rename(columns={'prob':'prob_cb','diff':'diff_cb'}, inplace=True)\n",
    "dfr=pd.concat([df_gbc[['date','course', 'marketTime', 'horseName', 'runners', 'decimalPrice', 'res_win', 'res_place', 'odds', 'C', 'prob_gbc', 'diff_gbc']],\n",
    "           df_xgb[['prob_xgb', 'diff_xgb']],df_cb[['prob_cb', 'diff_cb']] ], axis=1)\n",
    "dfr['wag_gbc']=np.where(dfr['diff_gbc']>=0.3,1,0)\n",
    "dfr['wag_xgb']=np.where(dfr['diff_xgb']>=1.5,1,0)\n",
    "dfr['wag_cb']=np.where(dfr['diff_cb']>=1.5,1,0)\n",
    "dfr=dfr.loc[~((dfr['diff_gbc']<0.2) & (dfr['diff_xgb']<1) & (dfr['diff_cb']<1))]\n",
    "dfr.to_csv(f'data/today_res.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = joblib.load('models/gbc.sav')\n",
    "gb.feature_importances_\n",
    "nums=gb.feature_importances_.reshape(-1,1)\n",
    "features=np.array(cols_numerical+cols_categorical).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                        features             importance\n17                 res_win_j_avg  9.612719740077693e-05\n32            res_win_j_avg_diff   9.52135932980455e-05\n41                       OR_diff  7.800700995077393e-05\n23           position_diff_j_avg  7.199990191130053e-05\n54  decimalPrice_diff_h_avg_rank  6.554663689137319e-05\n..                           ...                    ...\n37             decimalPrice_diff                    0.0\n34  decimalPrice_diff_j_avg_diff                    0.0\n1                  res_win_h_avg                    0.0\n22                 OR_diff_j_avg                    0.0\n0                   metric_h_avg                    0.0\n\n[66 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>features</th>\n      <th>importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>17</th>\n      <td>res_win_j_avg</td>\n      <td>9.612719740077693e-05</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>res_win_j_avg_diff</td>\n      <td>9.52135932980455e-05</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>OR_diff</td>\n      <td>7.800700995077393e-05</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>position_diff_j_avg</td>\n      <td>7.199990191130053e-05</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>decimalPrice_diff_h_avg_rank</td>\n      <td>6.554663689137319e-05</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>decimalPrice_diff</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>decimalPrice_diff_j_avg_diff</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>res_win_h_avg</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>OR_diff_j_avg</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>metric_h_avg</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>66 rows Ã— 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "df_features=pd.DataFrame(np.hstack([features,nums]), columns=['features','importance'])\n",
    "df_features.sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-d3998f75f237>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimportance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.bar(range(X_train.shape[1]), df_features.importance)\n",
    "plt.xticks(range(X_train.shape[1]), df_features.features)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(91,)"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "validate_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 TEST ACC:  0.90847963 VAL ACC:  1.0\n"
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "num_examples, n_inputs  = train_x.shape\n",
    "n_hidden1 = 100 \n",
    "n_hidden2 = 100 \n",
    "n_outputs = 2 \n",
    "learning_rate = 0.01\n",
    "n_epochs=5\n",
    "batch_size=64\n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "    my_batch_norm_layer = partial(\n",
    "            tf.layers.batch_normalization,\n",
    "            training=training,\n",
    "            momentum=batch_norm_momentum)\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    init.run() \n",
    "    for epoch in range (n_epochs): \n",
    "        for X_batch, y_batch in shuffle_batch(train_x, train_y, batch_size): \n",
    "            #X_batch_scaled = (X_batch - means) / stds\n",
    "            sess.run([training_op, extra_update_ops], feed_dict= {training: True, X: X_batch, y: y_batch } ) \n",
    "        if epoch % 5 == 0:\n",
    "            accuracy_test = accuracy.eval(feed_dict={X: test_x, y: test_y } ) \n",
    "            accuracy_val = accuracy.eval(feed_dict={X: validate_x, y: validate_y } ) \n",
    "            print (epoch, \"TEST ACC: \", accuracy_test,\"VAL ACC: \", accuracy_val ) \n",
    "    save_path = saver.save(sess, \"./models/my_model_final.ckpt\" ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python361064bittfenvcondac473d925d20c406f90cf4067bfe755b2",
   "display_name": "Python 3.6.10 64-bit ('tfenv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}